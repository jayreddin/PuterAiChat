{"chunk":45,"numChunks":47,"fileHash":"DS6QWbTdrmdJEfNC8Ec7aEISJw5n4IF3kMzfDlJvsks=","filePath":"attached_assets/Puterexamples.md","content":"\\<html\\>  \n\\<body\\>  \n    \\<script src=\"https://js.puter.com/v2/\"\\>\\</script\\>  \n    \\<script\\>  \n        puter.ai.chat(\"Analyze the potential impact of quantum computing on cryptography\", {  \n            model: 'claude-3-5-sonnet'  \n        }).then(response \\=\\> {  \n            document.write(response);  \n        });  \n    \\</script\\>  \n\\</body\\>  \n\\</html\\>\n\nThis example shows how to specify a different model using the options parameter. Claude 3.5 Sonnet is well-suited for tasks requiring deep analysis or technical understanding.\n\n## **Example 3**\n\n## Streaming Responses with Llama\n\nFor longer responses, you can use streaming to get results in real-time:\n\n\\<html\\>  \n\\<body\\>  \n    \\<script src=\"https://js.puter.com/v2/\"\\>\\</script\\>  \n    \\<script\\>  \n        async function streamResponse() {  \n            const response \\= await puter.ai.chat(  \n                \"Write a detailed analysis of renewable energy sources\",   \n                {  \n                    model: 'meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo',  \n                    stream: true  \n                }  \n            );  \n              \n            for await (const part of response) {  \n                document.write(part?.text);  \n            }  \n        }\n\n        streamResponse();  \n    \\</script\\>  \n\\</body\\>  \n\\</html\\>\n\nThis example demonstrates streaming with Llama, which is particularly useful for longer responses. The streaming approach provides a better user experience by showing the response as it's generated rather than waiting for the complete response.\n\n## **Example 4**\n\n## Vision Capabilities\n\nYou can also analyze images using GPT-4 Vision:\n\n\\<html\\>  \n\\<body\\>  \n"}